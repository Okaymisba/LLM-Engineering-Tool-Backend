from prompt_generation.prompt_generation import generate_prompt
from prompt_generation.query_local_model import query_local_model
from response.anthropic.query_anthropic_model import query_anthropic_model
from response.deepseek.query_deepseek_model import query_deepseek_model
from response.google.query_google_model import query_google_model
from response.openai.query_openai_model import query_openai_model


def generate_response(provider: str, model: str, question: str, prompt_context=None, instructions=None):
    """
    Generates a response from the given AI model and provider based on the input question,
    prompt context, and optional instructions. The function determines the appropriate
    query mechanism based on the specified provider and delegates the response generation
    task accordingly.

    :param provider: The name of the AI model provider, such as 'deepseek', 'openai',
        'anthropic', 'google', etc. Determines the specific model querying mechanism.
    :type provider: str

    :param model: The identifier or name of the model to be queried. It specifies the
        specific learning model under the provider to be used for generating the response.
    :type model: str

    :param question: The main question or input for which a response is to be generated.
        This query serves as the primary input for the AI model.
    :type question: str

    :param prompt_context: An optional parameter providing contextual information that
        may help in refining or tailoring the model's response.
    :type prompt_context: str or None

    :param instructions: Optional directives or specific instructions to guide the model
        in generating a more precise or targeted response.
    :type instructions: str or None

    :return: The response generated by the queried AI model based on the input parameters.
    :return type: str
    """
    if provider == "deepseek":
        return query_deepseek_model(model, question, prompt_context, instructions)
    elif provider == "openai":
        return query_openai_model(model, question, prompt_context, instructions)
    elif provider == "anthropic":
        return query_anthropic_model(model, question, prompt_context, instructions)
    elif provider == "google":
        return query_google_model(model, question, prompt_context, instructions)
    else:
        return query_local_model(generate_prompt(question, prompt_context, instructions))
